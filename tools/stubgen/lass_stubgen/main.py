# *** BEGIN LICENSE INFORMATION ***
#
# The contents of this file are subject to the Common Public Attribution License
# Version 1.0 (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
# https://lass.cocamware.com/cpal-license. The License is based on the
# Mozilla Public License Version 1.1 but Sections 14 and 15 have been added to cover
# use of software over a computer network and provide for limited attribution for
# the Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis, WITHOUT
# WARRANTY OF ANY KIND, either express or implied. See the License for the specific
# language governing rights and limitations under the License.
#
# The Original Code is LASS - Library of Assembled Shared Sources.
#
# The Initial Developer of the Original Code is Bram de Greve and Tom De Muer.
# The Original Developer is the Initial Developer.
#
# All portions of the code written by the Initial Developer are:
# Copyright (C) 2025 the Initial Developer.
# All Rights Reserved.
#
# Contributor(s):
#
# Alternatively, the contents of this file may be used under the terms of the
# GNU General Public License Version 2 or later (the GPL), in which case the
# provisions of GPL are applicable instead of those above.  If you wish to allow use
# of your version of this file only under the terms of the GPL and not to allow
# others to use your version of this file under the CPAL, indicate your decision by
# deleting the provisions above and replace them with the notice and other
# provisions required by the GPL License. If you do not delete the provisions above,
# a recipient may use your version of this file under either the CPAL or the GPL.
#
# *** END LICENSE INFORMATION ***

"""Generate *.pyi stubs for Lass Python bindings using libclang."""

import functools
import hashlib
import multiprocessing
import subprocess
import sys
from argparse import ArgumentParser
from pathlib import Path

from .generator import StubGenerator
from .parser import ParseError, Parser
from .stubdata import DuplicateError, StrPath, StubData


def main(argv: list[str]) -> int:
    parser = ArgumentParser(description=__doc__)
    parser.add_argument(
        "path", nargs="+", help="Source files with Lass Python bindings to parse"
    )
    parser.add_argument(
        "-I",
        "--include-directory",
        dest="include_dirs",
        metavar="<dir>",
        action="append",
        default=[],
        help="Additional include directories for libclang",
    )
    parser.add_argument(
        "-isystem",
        dest="system_include_dirs",
        metavar="<dir>",
        action="append",
        default=[],
        help="Additional system include directories for libclang",
    )
    parser.add_argument(
        "-D",
        "--define-macro",
        dest="defines",
        metavar="<define>",
        action="append",
        default=[],
        help="Preprocessor defines for libclang",
    )
    parser.add_argument(
        "--arg",
        dest="args",
        metavar="<arg>",
        action="append",
        default=[],
        help="Additional arguments for libclang",
    )
    parser.add_argument(
        "--output-dir",
        metavar="<output-dir>",
        type=str,
        help="Output directory for generated stubs",
    )
    parser.add_argument(
        "--package",
        metavar="<package>",
        type=str,
        help="""Package name that the module belongs to. It will be prepended to the
module name to form the fully qualified name of the module.""",
    )
    parser.add_argument(
        "--export",
        metavar="<json-file>",
        type=str,
        help="""Export stub data to JSON file. This can later be imported in another
lass_stubgen run using the --import option.""",
    )
    parser.add_argument(
        "--import",
        dest="imports",
        metavar="<json-file>",
        action="append",
        default=[],
        help="""Import previously exported stub data from JSON file. This imported data
will be used to resolve C++ names from modules currently being parsed. This is useful
for parsing modules that depend on other modules that have already been parsed. The
imported modules will be marked as imported and no *.pyi stub files will be generated
for them. The imported modules will be added to the current module's imports list.
This option can be used multiple times to import multiple JSON files.""",
    )
    parser.add_argument(
        "--obj",
        dest="object_files",
        metavar="<obj-file>",
        action="append",
        default=[],
        help="""Paths to object files generated by C++ compiler from source files.
When provided, parsed stub data will be cached in a JSON file in the cache directory.
The source file will only be parsed if the cached JSON file is older than the object
file. This option can be used multiple times to specify multiple object files. The
object fiels are matched to source files by their stem (the part of the filename before
the extension). No two object files can have the same stem.""",
    )
    parser.add_argument(
        "--cache-dir",
        metavar="<cache-dir>",
        type=str,
        help="""Cache directory for storing parsed stub data in JSON format. This is
only used when --obj is provided. If no --cache-dir is provided, the cache directory
will be created in the current working directory under the name .lass_stubgen_cache.""",
    )
    parser.add_argument(
        "--num-threads",
        metavar="<num-threads>",
        type=int,
        default=0,
        help="""Parse source files in paralel with maximum <num-threads> parallel jobs.
If not provided, the number of threads will be set to the number of CPU cores.""",
    )
    parser.add_argument(
        "--with-signatures",
        action="store_true",
        default=False,
        help="Add C++ signatures as comments to the generated stubs, for debugging",
    )
    parser.add_argument(
        "--debug-connect", type=int, default=0, help="Port to connect to for debugging"
    )

    args = parser.parse_args(argv)

    if args.debug_connect:
        import debugpy  # type: ignore # noqa: I001

        debugpy.connect(args.debug_connect)

    try:
        stubdata: StubData = parse(
            source_paths=args.path,
            object_files=args.object_files,
            include_dirs=args.include_dirs,
            system_include_dirs=args.system_include_dirs,
            defines=args.defines,
            args=args.args,
            package=args.package,
            imports=args.imports,
            export=args.export,
            cache_dir=args.cache_dir,
            num_threads=args.num_threads,
            parser_type=Parser,
        )
    except ParseError as err:
        for error in err.errors:
            print(error, file=sys.stderr)
        for note in err.__notes__:
            print(note, file=sys.stderr)
        print(f"{len(err.errors)} parse errors found, aborting", file=sys.stderr)
        return 1
    except DuplicateError as err:
        print(f"Error: {str(err)}", file=sys.stderr)
        return 1

    generate(stubdata, output_dir=args.output_dir, with_signature=args.with_signatures)
    return 0


def parse(
    source_paths: list[StrPath],
    *,
    object_files: list[StrPath] | None = None,
    include_dirs: list[StrPath] | None = None,
    system_include_dirs: list[StrPath] | None = None,
    defines: list[str] | None = None,
    args: list[str] | None = None,
    package: str | None = None,
    imports: list[StrPath] | None = None,
    export: StrPath | None = None,
    cache_dir: StrPath | None = None,
    num_threads: int = 0,
    parser_type: type[Parser] = Parser,
) -> StubData:
    """
    Parse the given source files and return a StubData object.
    """
    source_paths_ = [Path(path) for path in source_paths]
    cache_dir_ = Path(cache_dir) if cache_dir else Path.cwd() / ".stubgen_cache"
    imports_ = [Path(path) for path in imports or []]

    if object_files:
        object_files_map = {_name(path): Path(path) for path in object_files}
        if len(object_files_map) != len(object_files):
            raise ValueError(
                "Duplicate object file stems found in the list of object files"
            )
    else:
        object_files_map = {}

    if cmake_pch := object_files_map.get("cmake_pch"):
        # we have a precompiled header, let's parse that one first, and pre-load it
        # for all others.

        parent = cmake_pch.parent
        pch_header = parent / "cmake_pch.hxx"
        if not pch_header.exists():
            # multi-config build? let's try harder ...
            build_type = parent.name
            parent = parent.parent
            target_dir = parent.name
            parent = parent.parent
            pch_header = (
                parent / "CMakeFiles" / target_dir / build_type / "cmake_pch.hxx"
            )
        assert pch_header.exists(), f"Precompiled header {pch_header} does not exist"
        h = hashlib.sha1(str(pch_header.absolute()).encode("utf-8")).hexdigest()
        pch_path = cache_dir_ / f"{pch_header.name}.{h}.pch"
    else:
        pch_header = None
        pch_path = None

    parser = functools.partial(
        _parse_file,
        object_files_map=object_files_map,
        include_dirs=include_dirs,
        system_include_dirs=system_include_dirs,
        defines=defines,
        args=args,
        package=package,
        cache_dir=cache_dir_,
        pch_path=pch_path,
        parser_type=parser_type,
    )

    stubdata = StubData(package=package)
    for import_ in imports_:
        import_data = StubData.load(import_, imported=True)
        stubdata.update(import_data)

    if pch_header:
        # we have a precompiled header, let's parse that one first, and pre-load it
        parser(pch_header, save_pch=True)

    if num_threads == 1:
        for path in source_paths_:
            stubdata.update(parser(path))
    else:
        if not num_threads:
            num_threads = multiprocessing.cpu_count()
        num_threads = min(num_threads, len(source_paths_)) or 1
        with multiprocessing.Pool(num_threads) as pool:
            for stubdata_ in pool.imap_unordered(parser, source_paths_):
                stubdata.update(stubdata_)

    stubdata.fix_fully_qualified_names()

    if export:
        stubdata.dump(export)

    return stubdata


def _parse_file(
    source_path: Path,
    *,
    package: str | None,
    include_dirs: list[StrPath] | None,
    system_include_dirs: list[StrPath] | None,
    defines: list[str] | None,
    args: list[str] | None,
    object_files_map: dict[str, Path],
    cache_dir: Path,
    pch_path: StrPath | None = None,
    save_pch: bool = False,
    parser_type: type[Parser],
) -> StubData:
    try:
        object_file = object_files_map.get(_name(source_path))
        if object_file:
            h = hashlib.sha1(str(source_path.absolute()).encode("utf-8")).hexdigest()
            cache_file = cache_dir / f"{source_path.name}.{h}.json"

            try:
                cache_mtime = cache_file.stat().st_mtime
                object_mtime = object_file.stat().st_mtime
            except FileNotFoundError:
                pass
            else:
                if cache_mtime > object_mtime:
                    stubdata = StubData.load(cache_file)
                    return stubdata
        else:
            cache_file = None

        print(f"Parsing {source_path}...", file=sys.stderr)
        parser = parser_type(
            package=package,
            include_dirs=include_dirs,
            system_include_dirs=system_include_dirs,
            defines=defines,
            args=args,
            pch_path=pch_path,
        )
        parser.parse(source_path, save_pch=save_pch)
        stubdata = parser.stubdata

        if cache_file:
            stubdata.dump(cache_file)

        return stubdata
    except Exception as err:
        err.add_note(f"While parsing file {source_path}")
        raise err


def _name(path: StrPath) -> str:
    """Return name of file without any extension, so before first dot"""
    return Path(path).name.split(".")[0]


def generate(
    stubdata: StubData,
    *,
    output_dir: StrPath | None = None,
    with_signature: bool = False,
) -> None:
    output_dir_ = Path(output_dir) if output_dir else None
    generator = StubGenerator(stubdata)
    for mod_def in stubdata.modules.values():
        if mod_def.imported:
            continue
        if output_dir_:
            output_dir_.mkdir(parents=True, exist_ok=True)
            output_file = output_dir_ / f"{mod_def.py_name}.pyi"
            print(f"Writing {output_file}...", file=sys.stderr)
            with open(output_file, "w") as file:
                generator.write_module(
                    mod_def, file=file, with_signature=with_signature
                )
            reformat(output_file)
        else:
            print(f"# ============= {mod_def.py_name} =============", file=sys.stderr)
            generator.write_module(mod_def, file=sys.stdout)


def reformat(file: Path) -> None:
    """
    Reformat the file using ruff
    """
    bin_dir = Path(sys.executable).parent
    ruff = bin_dir / "ruff"
    subprocess.run([ruff, "check", "--fix-only", "--silent", file], check=True)
    subprocess.run([ruff, "format", "--silent", file], check=True)
